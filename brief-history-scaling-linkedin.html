<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>A Brief History of Scaling LinkedIn | Hoanh An</title><meta name="generator" content="Jekyll v3.7.4" /><meta property="og:title" content="A Brief History of Scaling LinkedIn" /><meta name="author" content="Hoanh An" /><meta property="og:locale" content="en_US" /><meta name="description" content="Started as a single monolithic application, Leo, that hosted various pages, handled business logic and connected to a handful of databases Needed to manage a network of member connections and scale independent of Leo so built a new system for their member graph Used Java RPC for communication, Apache Lucene for search capabilities Introduced replica DBs as the site grew To keep replica DBs in sync, built data capture system, Databus, then open-sourced it Observed that Leo was often going down in production, difficult for the team to troubleshoot, recover, release new code Killed Leo Broke it up into many small services Frontend: fetch data models from different domains, presentation logic Mid-tier: provide API access to data models and add more layer of cache (memcache/couchbase/Voldemort) Backend: provide consistent access to its database Developed data pipelines for streaming and queueing data that later became Apache Kafka Empowered Hadoop jobs Built realtime analytics Improved monitoring and alerting In 2011, kicked off an internal initiative, Inversion Paused on feature development Focused on improving tooling and deployment, infrastructure, and developer productivity Got rid of Jave RPC because it was inconsistent across team as well as tightly coupled and built Rest.li for a more scalable RESTful architecture Since fetching many types of different data and making hundreds of downstream calls made the “call graph” difficult to manage, the team grouped multiple services together to allow a single access API Had a specific team optimize the block Scaled to 3 main data centers and multiple PoP around the globe in 2015 Developed an offline workflow using Hadoop to precompute data insights Rethought frontend approach Added client template, Dust.js, to the mix Cached templates in CDNs and browsers Adapted BigPipe and Play Framework for an async experience Introduced multiple tiers of proxies, Apache Traffic Server, HAProxy, to handle load balancing, data center pinning, security, intelligent routing, server side rendering,… Utilized optimized hardware, advanced memory and system tuning, and newer Java runtimes" /><meta property="og:description" content="Started as a single monolithic application, Leo, that hosted various pages, handled business logic and connected to a handful of databases Needed to manage a network of member connections and scale independent of Leo so built a new system for their member graph Used Java RPC for communication, Apache Lucene for search capabilities Introduced replica DBs as the site grew To keep replica DBs in sync, built data capture system, Databus, then open-sourced it Observed that Leo was often going down in production, difficult for the team to troubleshoot, recover, release new code Killed Leo Broke it up into many small services Frontend: fetch data models from different domains, presentation logic Mid-tier: provide API access to data models and add more layer of cache (memcache/couchbase/Voldemort) Backend: provide consistent access to its database Developed data pipelines for streaming and queueing data that later became Apache Kafka Empowered Hadoop jobs Built realtime analytics Improved monitoring and alerting In 2011, kicked off an internal initiative, Inversion Paused on feature development Focused on improving tooling and deployment, infrastructure, and developer productivity Got rid of Jave RPC because it was inconsistent across team as well as tightly coupled and built Rest.li for a more scalable RESTful architecture Since fetching many types of different data and making hundreds of downstream calls made the “call graph” difficult to manage, the team grouped multiple services together to allow a single access API Had a specific team optimize the block Scaled to 3 main data centers and multiple PoP around the globe in 2015 Developed an offline workflow using Hadoop to precompute data insights Rethought frontend approach Added client template, Dust.js, to the mix Cached templates in CDNs and browsers Adapted BigPipe and Play Framework for an async experience Introduced multiple tiers of proxies, Apache Traffic Server, HAProxy, to handle load balancing, data center pinning, security, intelligent routing, server side rendering,… Utilized optimized hardware, advanced memory and system tuning, and newer Java runtimes" /><link rel="canonical" href="https://hoanhan101.github.io/brief-history-scaling-linkedin" /><meta property="og:url" content="https://hoanhan101.github.io/brief-history-scaling-linkedin" /><meta property="og:site_name" content="Hoanh An" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-03-24T00:00:00-04:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="A Brief History of Scaling LinkedIn" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Hoanh An" /> <script type="application/ld+json"> {"url":"https://hoanhan101.github.io/brief-history-scaling-linkedin","mainEntityOfPage":{"@type":"WebPage","@id":"https://hoanhan101.github.io/brief-history-scaling-linkedin"},"author":{"@type":"Person","name":"Hoanh An"},"headline":"A Brief History of Scaling LinkedIn","dateModified":"2020-03-24T00:00:00-04:00","datePublished":"2020-03-24T00:00:00-04:00","description":"Started as a single monolithic application, Leo, that hosted various pages, handled business logic and connected to a handful of databases Needed to manage a network of member connections and scale independent of Leo so built a new system for their member graph Used Java RPC for communication, Apache Lucene for search capabilities Introduced replica DBs as the site grew To keep replica DBs in sync, built data capture system, Databus, then open-sourced it Observed that Leo was often going down in production, difficult for the team to troubleshoot, recover, release new code Killed Leo Broke it up into many small services Frontend: fetch data models from different domains, presentation logic Mid-tier: provide API access to data models and add more layer of cache (memcache/couchbase/Voldemort) Backend: provide consistent access to its database Developed data pipelines for streaming and queueing data that later became Apache Kafka Empowered Hadoop jobs Built realtime analytics Improved monitoring and alerting In 2011, kicked off an internal initiative, Inversion Paused on feature development Focused on improving tooling and deployment, infrastructure, and developer productivity Got rid of Jave RPC because it was inconsistent across team as well as tightly coupled and built Rest.li for a more scalable RESTful architecture Since fetching many types of different data and making hundreds of downstream calls made the “call graph” difficult to manage, the team grouped multiple services together to allow a single access API Had a specific team optimize the block Scaled to 3 main data centers and multiple PoP around the globe in 2015 Developed an offline workflow using Hadoop to precompute data insights Rethought frontend approach Added client template, Dust.js, to the mix Cached templates in CDNs and browsers Adapted BigPipe and Play Framework for an async experience Introduced multiple tiers of proxies, Apache Traffic Server, HAProxy, to handle load balancing, data center pinning, security, intelligent routing, server side rendering,… Utilized optimized hardware, advanced memory and system tuning, and newer Java runtimes","@type":"BlogPosting","@context":"https://schema.org"}</script><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Hoanh An" href="/atom.xml"><link rel="alternate" title="Hoanh An" type="application/json" href="https://hoanhan101.github.io/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style type="text/css"> body{font-family:-apple-system,BlinkMacSystemFont,'avenir next',avenir,Noto Sans,helvetica,'helvetica neue',sans-serif}h1{font-size:36px}body{font-size:1rem;line-height:1.5;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility}a,a:visited{color:inherit}a:hover,a:visited:hover{color:dodgerblue}li{list-style-position:outside;margin-left:18px;margin-bottom:2px}blockquote{background:#f9f9f9;border-left:5px solid black;font-size:100%;margin:2rem 0;padding:1rem}blockquote p{margin:0}blockquote footer{font-size:80%;margin:1rem 0 0 0}dl dt{margin-bottom:0.5rem}dl dd{font-style:italic;margin-bottom:2rem}code,.highlight{overflow:auto}code{word-break:break-all}pre{padding:1em}.date{opacity:0.6}.highlight table td{padding:5px}.highlight table pre{margin:0}.highlight .gh{color:#999999}.highlight .sr{color:#f6aa11}.highlight .go{color:#888888}.highlight .gp{color:#555555}.highlight .gu{color:#aaaaaa}.highlight .nb{color:#f6aa11}.highlight .cm{color:#75715e}.highlight .cp{color:#75715e}.highlight .c1{color:#75715e}.highlight .cs{color:#75715e}.highlight .c,.highlight .ch,.highlight .cd,.highlight .cpf{color:#75715e}.highlight .err{color:#960050}.highlight .gr{color:#960050}.highlight .gt{color:#960050}.highlight .gd{color:#49483e}.highlight .gi{color:#49483e}.highlight .ge{color:#49483e}.highlight .kc{color:#66d9ef}.highlight .kd{color:#66d9ef}.highlight .kr{color:#66d9ef}.highlight .no{color:#66d9ef}.highlight .kt{color:#66d9ef}.highlight .mf{color:#ae81ff}.highlight .mh{color:#ae81ff}.highlight .il{color:#ae81ff}.highlight .mi{color:#ae81ff}.highlight .mo{color:#ae81ff}.highlight .m,.highlight .mb,.highlight .mx{color:#ae81ff}.highlight .sc{color:#ae81ff}.highlight .se{color:#ae81ff}.highlight .ss{color:#ae81ff}.highlight .sd{color:#e6db74}.highlight .s2{color:#e6db74}.highlight .sb{color:#e6db74}.highlight .sh{color:#e6db74}.highlight .si{color:#e6db74}.highlight .sx{color:#e6db74}.highlight .s1{color:#e6db74}.highlight .s,.highlight .sa,.highlight .dl{color:#e6db74}.highlight .na{color:#a6e22e}.highlight .nc{color:#a6e22e}.highlight .nd{color:#a6e22e}.highlight .ne{color:#a6e22e}.highlight .nf,.highlight .fm{color:#a6e22e}.highlight .vc{color:#ffffff;background-color:#272822}.highlight .nn{color:#ffffff;background-color:#272822}.highlight .nl{color:#ffffff;background-color:#272822}.highlight .ni{color:#ffffff;background-color:#272822}.highlight .bp{color:#ffffff;background-color:#272822}.highlight .vg{color:#ffffff;background-color:#272822}.highlight .vi{color:#ffffff;background-color:#272822}.highlight .nv,.highlight .vm{color:#ffffff;background-color:#272822}.highlight .w{color:#ffffff;background-color:#272822}.highlight{color:#ffffff;background-color:#272822}.highlight .n,.highlight .py,.highlight .nx{color:#ffffff;background-color:#272822}.highlight .ow{color:#f92672}.highlight .nt{color:#f92672}.highlight .k,.highlight .kv{color:#f92672}.highlight .kn{color:#f92672}.highlight .kp{color:#f92672}.highlight .o{color:#f92672}hr{margin-top:20px;border:0;border-top:1px solid #eee}html{box-sizing:border-box;margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}body{background-color:#edf2f7;color:#495057}header,main{margin:0 auto;max-width:50rem}main{background:white;border:2px solid #e9ecef;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);padding:2rem 3rem}ul,ol{padding:0;list-style-position:inside}table{border-collapse:collapse;text-align:left;width:100%}table tr{border-bottom:1px solid black}table td{padding:0.5rem}img{width:100%;margin:0.5rem 0}nav ul{list-style:none !important;padding:0;text-align:center}nav ul li{display:inline-block}nav a,nav a:visited{margin:0.5rem;text-decoration:none;font-size:0.875rem;text-transform:uppercase;color:inherit}footer{margin:1rem 0;text-align:center;color:inherit}@media only screen and (max-width: 600px){nav a{margin:0}h1{font-size:26px}}</style>
    <!-- Google Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-155884327-1', 'auto');
	ga('send', 'pageview', { 'page': location.pathname + location.search + location.hash});
	ga('set', 'anonymizeIp', false);
    </script>
    <!-- End Google Analytics -->
    </head><body> <span class="over"></span><header role="banner"><nav role="navigation"><ul><li><a href="/" >Home</a></li><li><a href="/filter" >Filter</a></li><li><a href="/search">Search</a></li><li><a href="/about" >About</a></li><li><a href="https://mailchi.mp/579f9d6927dd/hoanhanco">Subscribe</a></li></ul></nav></header><main id="main" role="main"><article role="article"><h1>A Brief History of Scaling LinkedIn</h1><p>How they started in 2003, had 2700 members the first week until 2015 when they served more then 350 millions members.</p><time class="date" datetime="2020-03-24T00:00:00-04:00"> March 24, 2020 &middot; 1 min read &middot; <a class="post" href="/category/System-design-notes">System design notes</a><hr> </time><ul><li>Started as a single monolithic application, Leo, that hosted various pages, handled business logic and connected to a handful of databases</li><li>Needed to manage a network of member connections and scale independent of Leo so built a new system for their member graph<ul><li>Used Java RPC for communication, Apache Lucene for search capabilities</li></ul></li><li>Introduced replica DBs as the site grew<ul><li>To keep replica DBs in sync, built data capture system, Databus, then open-sourced it</li></ul></li><li>Observed that Leo was often going down in production, difficult for the team to troubleshoot, recover, release new code<ul><li>Killed Leo</li><li>Broke it up into many small services<ul><li>Frontend: fetch data models from different domains, presentation logic</li><li>Mid-tier: provide API access to data models and add more layer of cache (memcache/couchbase/Voldemort)</li><li>Backend: provide consistent access to its database</li></ul></li></ul></li><li>Developed data pipelines for streaming and queueing data that later became Apache Kafka<ul><li>Empowered Hadoop jobs</li><li>Built realtime analytics</li><li>Improved monitoring and alerting</li></ul></li><li>In 2011, kicked off an internal initiative, Inversion<ul><li>Paused on feature development</li><li>Focused on improving tooling and deployment, infrastructure, and developer productivity</li></ul></li><li>Got rid of Jave RPC because it was inconsistent across team as well as tightly coupled and built Rest.li for a more scalable RESTful architecture</li><li>Since fetching many types of different data and making hundreds of downstream calls made the “call graph” difficult to manage, the team grouped multiple services together to allow a single access API<ul><li>Had a specific team optimize the block</li></ul></li><li>Scaled to 3 main data centers and multiple PoP around the globe in 2015</li><li>Developed an offline workflow using Hadoop to precompute data insights</li><li>Rethought frontend approach<ul><li>Added client template, Dust.js, to the mix</li><li>Cached templates in CDNs and browsers</li><li>Adapted BigPipe and Play Framework for an async experience</li></ul></li><li>Introduced multiple tiers of proxies, Apache Traffic Server, HAProxy, to handle load balancing, data center pinning, security, intelligent routing, server side rendering,…</li><li>Utilized optimized hardware, advanced memory and system tuning, and newer Java runtimes</li></ul><hr /><p><strong>References:</strong></p><ul><li><a href="https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin">https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin</a></li></ul><hr><div class="related"><h4>Interested in getting updates on my latest insights and projects, feel free to join my <a class="post" href="https://mailchi.mp/579f9d6927dd/hoanhanco">Insights mailing list</a>. Otherwise, checkout similar posts here:</h4><ul><li><a href="/design-data-intensive-apps">Designing Data-Intensive Applications by Martin Kleppmann</a></li><li><a href="/web-architectural-components">Modern Web Architectural Components</a></li><li><a href="/serverless-aws-lamda">Serverless architecture with AWS Lamda</a></li></ul></div><hr><p> Tagged: <a class="post" href="/tag/microservices">#microservices</a>, <a class="post" href="/tag/linkedin">#linkedin</a></p><br><div id="disqus_thread"></div><script> var disqus_name = "hoanhan101"; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_name + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></article></main><footer class="footer" role="contentinfo"> <small> © 2020 <a style="text-decoration: none;" href="/about">Hoanh An</a> | <a style="text-decoration: none;" href="https://www.linkedin.com/in/hoanhan101/">LinkedIn</a> | <a style="text-decoration: none;" href="https://github.com/hoanhan101">GitHub</a> | <a style="text-decoration: none;" href="mailto:hoanhan101@gmail.com">Email</a> </small></footer></body></html>
